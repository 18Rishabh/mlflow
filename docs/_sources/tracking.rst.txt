.. _tracking:

MLflow Tracking
===============

The MLflow Tracking component lets you log and query experiments using either REST or Python.

Concepts
--------

MLflow Tracking is organized around the concept of *runs*, which are executions of some piece of
data science code. Each run records the following information:

Code Version
    Git commit used to execute the run, if it was executed from an :ref:`MLflow Project <projects>`.

Parameters
    Key-value input parameters of your choice. Both keys and values are strings.

Metrics
    Key-value metrics where the value is numeric. Each metric can be updated throughout the
    course of the run (for example, to track how your model's loss function is converging), and MLflow
    will record and let you visualize the metric's full history.

Artifacts
    Output files in any format. For example, you can record images (for example, PNGs), models
    (for example, a pickled SciKit-Learn model) or even data files (for example, a
    `Parquet <https://parquet.apache.org/>`_ file) as artifacts.

Runs can be recorded from anywhere you run your code through MLflow's Python or REST APIs: for
example, you can record them in a standalone program, on a remote cloud machine, or in an
interactive notebook. If you record runs in an :ref:`MLflow Project <projects>`, however, MLflow
remembers the project URI and source version.

Finally, runs can optionally be organized into *experiments*, which group together runs for a
specific task. You can set an experiment with :py:func:`mlflow.set_experiment_id` or the
corresponding REST parameters. The MLflow UI and API let you create and search for experiments.

Once your runs have been recorded, you can query them using the :ref:`tracking_ui` or the MLflow API.

Where Runs Get Recorded
-----------------------

MLflow runs can be recorded either locally in files or remotely to a Tracking Server.
By default, the MLflow Python API logs runs to files in an ``mlruns`` directory wherever you
ran your program. You can then run ``mlflow ui`` to see the logged runs. Set the
``MLFLOW_TRACKING_URI`` environment variable to a server's URI or call
:py:func:`mlflow.set_tracking_uri` to log runs remotely.

You can also :ref:`run your own tracking server <tracking_server>` to record runs.

Logging Data to Runs
--------------------

You can log data to runs using either the MLflow REST API or the Python API. In this section, we
show the Python API, but there are corresponding REST APIs as well.

Basic Logging Functions
^^^^^^^^^^^^^^^^^^^^^^^

:py:func:`mlflow.set_tracking_uri` connects to a tracking URI. You can also set the
`MLFLOW_TRACKING_URI` environment variable to have MLflow find a URI from there. In both cases,
the URI can either be a HTTP/HTTPS URI for a remote server, or a local path to log data to a
directory. The URI defaults to ``mlruns``.

:py:func:`mlflow.create_experiment` creates a new experiment and returns its ID. Runs can be
launched under the experiment by passing the experiment ID to `mlflow.start_run`

:py:func:`mlflow.start_run` starts a new run and returns a :py:class:`mlflow.tracking.Run`
object. You do not need to call `start_run` explicitly: calling one of the logging functions with
no active run will automatically start a new one.

:py:func:`mlflow.end_run` ends the currently active run, if any, taking an optional run status.

:py:func:`mlflow.active_run` returns the active Run object, if any.

:py:func:`mlflow.log_parameter` logs a key-value parameter in the currently active run. The keys and
values are both strings.

:py:func:`mlflow.log_metric` logs a key-value metric. The value must always be a number. MLflow will
remember the history of values for each metric.

:py:func:`mlflow.log_artifact` logs a local file as an artifact, optionally taking an
``artifact_path`` to place it in within the run's artifacts. Run artifacts can be organized into
directories, so you can place the artifact in a directory this way.

:py:func:`mlflow.log_artifacts` logs all the files in a given directory as artifacts, again taking
an optional ``artifact_path``.

:py:func:`mlflow.get_artifact_uri` returns the URI that artifacts from the current run should be
logged to, which can potentially be a distributed storage system URI (for example, ``s3://`` or
``dbfs://``). If your application supports writing to such URIs (for example, when using Apache Spark), it
can write artifacts directly to distributed storage this way.

Logging Large Artifacts
^^^^^^^^^^^^^^^^^^^^^^^

Although many artifacts you might log in MLflow are small files, it is also possible and use MLflow
to record large datasets, such as a computed feature. In this case, we recommend using
:py:func:`mlflow.get_artifact_uri` to write data directly to a distributed storage system, without
having to stream it through an MLflow server. MLflow currently only supports ``s3://`` and
``dbfs://`` URIs, and is designed to provide artifact paths compatible with
`Apache Spark <https://spark.apache.org>`_ and
`Databricks Runtime <https://databricks.com/product/databricks-runtime>`_ so that you can use Spark
APIs to read and write distributed files directly. We recommend the
`Apache Parquet <https://parquet.apache.org>`_ format for large tabular datasets, as many data
science tools can read it.

Launching Multiple Runs in One Program
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sometimes you want to execute multiple MLflow runs in the same program: for example, maybe you are
performing a hyperparameter search locally or your experiments are just very fast to run. This is
easy to do because the Run object returned by :py:func:`mlflow.start_run` is a Python
`context manager <https://docs.python.org/2.5/whatsnew/pep-343.html>`_. You can "scope" each run to
just one block of code as follows:

.. code:: python

   with mlflow.start_run():
       mlflow.log_parameter("x", 1)
       mlflow.log_metric("y", 2)
       ...

The run will remain open throughout the ``with`` statement, and will automatically be closed when the
statement exits, even if it exits due to an exception.

Organizing Runs in Experiments
------------------------------

**TODO: Coming soon!**

.. _tracking_ui:

Tracking UI
-----------

The Tracking UI lets you visualize, search and compare runs, as well as download run artifacts or
metadata for analysis in other tools. If you have been logging runs to a local ``mlruns`` directory,
simply run ``mlflow ui`` in the directory above it, and it will load the corresponding runs.
Alternatively, the :ref:`MLflow server <tracking_server>` serves the same UI.

The UI contains the following key features:

**TODO: Coming soon!**

.. _tracking_query_api:

Querying Runs Programmatically
------------------------------

All of the functions in the Tracking UI can be accessed programmatically through the
:py:mod:`mlflow.tracking` module and the REST API. This makes it easy to do several
common tasks:

* Query and compare runs using any data analysis tool of your choice, for example, **pandas**.
* Determine the artifact URI for a run to feed some of its artifacts into a new run when executing
  a workflow.
* Load artifacts from past runs as :ref:`models`.
* Run automated parameter search algorithms, where you query the metrics from various runs to
  submit new ones.

.. _tracking_server:

Running a Tracking Server
-------------------------

**TODO: Coming soon!**
